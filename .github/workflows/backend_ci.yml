# .github/workflows/backend_ci.yml
name: Backend CI

on:
  workflow_dispatch: {}
  schedule:
    - cron: "30 4 * * *" # Daily at 04:00 UTC
  pull_request:
    branches: [master]
    paths:
      - "backend/**"
      - "supabase/**"
      - ".github/workflows/backend_ci.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash

jobs:
  #-------------------------------------------------------------#
  # 1) SQL-only linting (SQLFluff)                               #
  #-------------------------------------------------------------#
  lint-sql:
    if: ${{ github.event_name != 'schedule' }}
    name: Lint Hand-written SQL Migrations
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-sqlfluff-${{ hashFiles('backend/requirements.txt') }}

      - name: Install SQLFluff
        run: |
          python -m pip install --upgrade pip
          pip install "sqlfluff[jinja]"

      - name: Lint migrations
        run: |
          files=$(find supabase/migrations -maxdepth 1 -type f \
            -regextype posix-extended \
            -regex '.*/[0-9]{3}_.*\.sql' \
            | sort)
          [[ -z "$files" ]] && echo "No migrations to lint – exiting." && exit 0
          echo "$files" | sed 's/^/  • /'
          sqlfluff lint $files

  #-------------------------------------------------------------#
  # 2) Apply & smoke-test migrations in a local Supabase stack  #
  #-------------------------------------------------------------#
  test-migrations:
    if: ${{ github.event_name != 'schedule' }}
    name: Test DB Migrations & Views
    runs-on: ubuntu-latest
    needs: lint-sql
    steps:
      - name: ⏱ Wait 10 minutes before starting
        run: sleep 600

      - uses: actions/checkout@v4

      - name: Install Postgres client
        run: sudo apt-get update && sudo apt-get install -y postgresql-client

      - uses: supabase/setup-cli@v1
        with:
          version: latest

      - name: Start Supabase
        run: supabase start

      - name: Wait for Postgres
        run: |
          for _ in {1..10}; do
            pg_isready -h 127.0.0.1 -p 54322 -U postgres && break
            sleep 3
          done

      - name: Reset & apply migrations
        run: PGSSLMODE=disable supabase db reset

      - name: Refresh materialized view
        run: |
          PGPASSWORD=postgres psql -h 127.0.0.1 -p 54322 -U postgres -d postgres \
            -c "REFRESH MATERIALIZED VIEW public.team_rolling_20;"

      - name: Smoke-test view
        run: |
          PGPASSWORD=postgres psql -h 127.0.0.1 -p 54322 -U postgres -d postgres \
            -c "SELECT COUNT(*) FROM public.team_rolling_20;"

      - name: Stop Supabase
        if: always()
        run: supabase stop --no-backup

  #-------------------------------------------------------------#
  # 3) Python lint – diff-only                                  #
  #-------------------------------------------------------------#
  python-lint-diff:
    if: ${{ github.event_name != 'schedule' }}
    name: Ruff Lint (changed Python files only)
    runs-on: ubuntu-latest
    needs: test-migrations
    steps:
      - name: ⏱ Wait 20 minutes before starting
        run: sleep 1200

      - uses: actions/checkout@v4
        with:
          fetch-depth: 0 # ← added

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Ruff
        run: |
          python -m pip install --upgrade pip
          pip install ruff

      - name: Determine changed Python files
        id: diff
        run: |
          # If this is a manual dispatch, or we have no before/PR base, skip diff-based lint
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "No base ref on workflow_dispatch → skipping diff lint"
            echo "files=" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # For PRs use the PR base SHA; for pushes use the previous commit
          base_ref="${{ github.event.pull_request.base.sha || github.event.before }}"
          if [[ -z "$base_ref" ]]; then
            echo "No base ref found → skipping diff lint"
            echo "files=" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "Comparing against base ref: $base_ref"
          changed=$(git diff --name-only --diff-filter=ACMRT "$base_ref" HEAD -- 'backend/**/*.py' | tr '\n' ' ')
          echo "files=$changed" >> "$GITHUB_OUTPUT"

      - name: Run Ruff on changed files
        if: steps.diff.outputs.files != ''
        run: |
          echo "Files to lint:"
          echo "${{ steps.diff.outputs.files }}" | sed 's/^/  • /'
          ruff check ${{ steps.diff.outputs.files }}

      - name: Skip (no Python changes)
        if: steps.diff.outputs.files == ''
        run: echo "No Python files changed – skipping Ruff."

  #-------------------------------------------------------------#
  # 4) Backend deps install (+ optional pytest)                  #
  #-------------------------------------------------------------#
  test-python-backend:
    if: ${{ github.event_name != 'schedule' }}
    name: Backend Dependency Install (pytest optional)
    runs-on: ubuntu-latest
    needs: python-lint-diff
    steps:
      - name: ⏱ Wait 30 minutes before starting
        run: sleep 1800

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache Python deps
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-python-${{ hashFiles('backend/requirements.txt') }}

      - name: Install dependencies
        run: pip install -r backend/requirements.txt

      # - name: Run tests
      #   run: |
      #     cd backend
      #     pytest -q

  #-------------------------------------------------------------#
  # 5)  Run data-pipeline scripts that fetch + upsert to Supabase
  #-------------------------------------------------------------#
  run-data-scripts:
    if: ${{ github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' }}
    name: Run Data Fetch & Upsert Scripts
    runs-on: ubuntu-latest
    needs: test-python-backend
    steps:
      - name: ⏱ Wait 40 minutes before starting
        run: sleep 2400

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r backend/requirements.txt

      - name: Install Google Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # ──▶ run as many scripts as you like; each inherits the same env block
      - name: Run MLB preview ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/mlb_games_preview.py

      - name: Run NBA preview ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/nba_games_preview.py

      - name: Run NBA historical stats ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/nba_game_stats_historical.py

      - name: Run NBA player-stats historical ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/nba_player_stats_historical.py

      - name: Run MLB game-stats historical ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/mlb_game_stats_historical.py

      - name: Run NBA team-stats historical ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/nba_team_stats_historical.py

      - name: Run MLB team-stats historical ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: |
          python backend/data_pipeline/mlb_team_stats_historical.py

      - name: Run NBA injuries ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          API_SPORTS_KEY: ${{ secrets.API_SPORTS_KEY }}
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
          RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
          RAPIDAPI_HOST: ${{ secrets.RAPIDAPI_HOST }}
        run: |
          python backend/data_pipeline/nba_injuries.py
